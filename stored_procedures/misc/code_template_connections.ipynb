{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "<p><b>Author:</b> Nhan Tran\n",
    "<p><b>Updated:</b> 12/18/2023\n",
    "<p><b>Description:</b> Python code example for connecting to various data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name='toc'>Table of Content</a>\n",
    "\n",
    "<ul style='font-size:12pt;line-height:1.8em'>\n",
    "    <li><a href='#1_0'>1. Extract Credentail Using DataIku API</a></li>\n",
    "    <li><a href='#2_0'>2. FTP/SFTP Connection</a></li>\n",
    "    <ul>\n",
    "        <li><a href='#2_a'>2.a Option 1: Dataiku API</a></li>\n",
    "        <ul>\n",
    "            <li><a href='#2_a1'>2.a1 Read data from ftp</a></li>\n",
    "            <li><a href='#2_a2'>2.a2 Write df to ftp</a></li>\n",
    "            <li><a href='#2_a3'>2.a3 Read/Write pttx to and from ftp folder</a></li>\n",
    "            <li><a href='#2_a4'>2.a4 Read/write PDF to FTP folder</a></li>\n",
    "        </ul>\n",
    "        <li><a href='#2_b'>2.b Option 2: Python ftp</a></li>\n",
    "        <ul>\n",
    "            <li><a href='#2_b1'>2.b1 Established FTP connection</a></li>\n",
    "            <li><a href='#2_b2'>2.b2 Read data from ftp</a></li>\n",
    "            <li><a href='#2_b3'>2.b3 Write df to ftp</a></li>\n",
    "            <li><a href='#2_b4'>2.b4 Move data between ftp folder</a></li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    <li><a href='#3_0'>3. Snowflake Connection</a></li>\n",
    "    <ul>\n",
    "        <li><a href='#3_a'>3.a Option 1: Dataiku API</a></li>\n",
    "        <ul>\n",
    "            <li><a href='#3_a1'>3.a1 Read from Snowflake</a></li>\n",
    "            <li><a href='#3_a2'>3.a2 Write to Snowflake</a></li>\n",
    "        </ul>\n",
    "        <li><a href='#3_b'>3.b Option 2: Python Snowflake Connector</a></li>\n",
    "    </ul>\n",
    "    <li><a href='#4_0'>4. SQL Connection</a></li>\n",
    "    <ul>\n",
    "        <li><a href='#4_a'>4.a MySQL Connection</a></li>\n",
    "        <li><a href='#4_b'>4.b Oracle Connection</a></li>\n",
    "        <li><a href='#4_c'>4.c Connection</a></li>\n",
    "    </ul>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><a href='#toc' style='text-decoration:none;font-weight:bold;color:#0877ff;'>&#11014;&#65039; Back to the Top</a></div>\n",
    "\n",
    "# <a name='1_0'>1. Extract Credentail Using DataIku API</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataiku\n",
    "from dataiku import pandasutils as pdu\n",
    "import pandas as pd, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the default user for dataiku api lookup\n",
    "eid = 'E1724299'  \n",
    "\n",
    "# create dataiku api object\n",
    "# https://developer.dataiku.com/latest/concepts-and-examples/authinfo.html\n",
    "client = dataiku.api_client()\n",
    "user = client.get_user(eid)\n",
    "client_usr = user.get_client_as()\n",
    "ai = client_usr.get_auth_info(with_secrets=True)\n",
    "\n",
    "# get existing connection info\n",
    "dcon = client.list_connections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(fol_nm):\n",
    "    project = client.get_default_project()\n",
    "    \n",
    "    try: #checking if a given folder existed\n",
    "        folder = dataiku.Folder(fol_nm)\n",
    "        folder.get_info()\n",
    "        print('Folder Exist:', fol_nm)\n",
    "    except:\n",
    "        #create managed folder\n",
    "        mfol = project.create_managed_folder(fol_nm)\n",
    "        \n",
    "        #change the folder connection to FTP and set the directory path\n",
    "        settings = mfol.get_settings()\n",
    "        settings.set_connection_and_path(connection='VSFTP_Write', path='/_PHI_DataScienceShare/')\n",
    "        settings.save()\n",
    "        print('New folder created:', fol_nm)\n",
    "    return print('Folder check completed!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project = client.get_default_project()\n",
    "# folder = dataiku.Folder('PHI_DataScienceShare')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folderx = project.get_managed_folder('PHI_DataScienceShare')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><a href='#toc' style='text-decoration:none;font-weight:bold;color:#0877ff;'>&#11014;&#65039; Back to the Top</a></div>\n",
    "\n",
    "# <a name='2_0'>2. FTP/SFTP Connection</a>\n",
    "## <a name='2_a'>2.a Option 1: Dataiku API</a>\n",
    "### <a name='2_a1a'>2.a1a Read CSV data from FTP folder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://doc.dataiku.com/dss/11/python-api/managed_folders.html\n",
    "# https://doc.dataiku.com/dss/latest/connecting/managed_folders.html\n",
    "\n",
    "folder_name = 'PHI_DataScienceShare'\n",
    "# check if folder exist, if not create new\n",
    "# check_folder(folder_name)\n",
    "\n",
    "#create a folder object in flow then write this next part\n",
    "folder = dataiku.Folder(folder_name)\n",
    "lis = [x for x in folder.list_paths_in_partition() if 'test_file.csv' in x][0]\n",
    "\n",
    "print('Reading file: ',lis)\n",
    "df = pd.read_csv(folder.get_download_stream(lis))\n",
    "\n",
    "# dapi = dataiku.Dataset('tablename_inFlow')  #retrieve sf dataset obj\n",
    "# dapi.write_with_schema(df)  #write BMA data to sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_a1b'>2.a1b Read Excel data from FTP folder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = [x for x in folder.list_paths_in_partition() if 'test_file.xlsx' in x][0]\n",
    "df = pd.read_excel(folder.get_download_stream(lis).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_a2'>2.a2 Write df to FTP folder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "#write csv files\n",
    "buffer = io.StringIO()  #create textwrapper object\n",
    "df.to_csv(buffer, index=False)  #write data to textwrapper\n",
    "with io.BytesIO(str.encode(buffer.getvalue())) as f:  #convert txt into byte object\n",
    "    folder.upload_stream(\"test_filex.csv\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#write xlsx files\n",
    "buffer = io.BytesIO()  #create textwrapper object\n",
    "df.to_excel(buffer, index=False)  #write data to textwrapper\n",
    "with io.BytesIO(buffer.getvalue()) as f:  #convert txt into byte object\n",
    "    folder.upload_stream(\"test_filex2.xlsx\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_a3'>2.a3 Read/write pptx to and from FTP folder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "folder = dataiku.Folder('PHI_DataScienceShare')  #input folder\n",
    "\n",
    "# lis = [x for x in folder.list_paths_in_partition() if 'QA_MOR_DQC_template.pptx' in x][0]\n",
    "path = '/script_scheduled/QA_pptx_auto/'\n",
    "file = 'QA_MOR_DQC_template.pptx'\n",
    "root = Presentation(io.BytesIO(folder.get_download_stream(path+file).read()))\n",
    "\n",
    "# slide_layout = root.slide_layouts[1]\n",
    "# root.slides.add_slide(slide_layout)\n",
    "\n",
    "#write pptx files\n",
    "buffer = io.BytesIO()  #create textwrapper object\n",
    "root.save(buffer)  #write data to textwrapper\n",
    "\n",
    "filen = path + \"QA_MOR_DQC_test234.pptx\"\n",
    "with io.BytesIO(buffer.getvalue()) as f:  #convert txt into byte object\n",
    "    folder.upload_stream(filen, f)\n",
    "#     f.close()\n",
    "\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_a4'>2.a4 Read/write PDF to FTP folder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, tempfile\n",
    "from tabula import read_pdf\n",
    "import dataiku\n",
    "\n",
    "#this method is a workaround for folder.get_download_stream() not being seekable.\n",
    "def readPDF(folder, filename):\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        local_file_path = os.path.join(tmpdirname, os.path.basename(filename))\n",
    "        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n",
    "        with folder.get_download_stream(filename) as f_remote, open(local_file_path, 'wb') as f_local:\n",
    "            shutil.copyfileobj(f_remote, f_local)\n",
    "        df = pd.DataFrame(read_pdf(local_file_path, stream = True, area=(24, 30, 65, 80), relative_area=True)[0])\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "folder = dataiku.Folder(Folder_ID)\n",
    "for file_name in folder.list_paths_in_partition():\n",
    "    readPDF(folder, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><a href='#toc' style='text-decoration:none;font-weight:bold;color:#0877ff;'>&#11014;&#65039; Back to the Top</a></div>\n",
    "\n",
    "\n",
    "## <a name='2_b'>2.b Option 2: Python ftp library</a>\n",
    "### <a name='2_b1'>2.b1 Established FTP connection</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ftplib import FTP\n",
    "import os, io\n",
    "\n",
    "# create ftp connection\n",
    "ftp = FTP()\n",
    "ftp_cd = dcon['vsftp']['params']\n",
    "ftp.connect('vsftp', ftp_cd['port'])\n",
    "\n",
    "# usr = 'svc.dscidatiku'  #username for password lookup; read-only access\n",
    "usr = 'svc.datasciencetm'  #username for password lookup; read-write access\n",
    "\n",
    "pss = [k['value'] for k in ai[\"secrets\"] if k['key'] == usr][0]\n",
    "ftp.login(usr, pss)\n",
    "\n",
    "# ftp.cwd('/_Phi_Datascienceshare/script_scheduled/WCSL_pop_auto')\n",
    "ftp.dir()  #show default directory contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_b2'>2.b2 Read data from ftp</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv file\n",
    "ftp.cwd('/')  #BestPractice: set the current working directory\n",
    "filename = 'VSFTP - Virtual Directories.csv'  #filename\n",
    "\n",
    "download_file = io.BytesIO()  #create memory object\n",
    "ftp.retrbinary(\"RETR {}\".format(filename), download_file.write)  #retrieve file from ftp\n",
    "download_file.seek(0)  #load file into memory\n",
    "\n",
    "ref = pd.read_csv(download_file)  #read virtual file into pandas\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel file\n",
    "ftp.cwd('/_Phi_DataScienceShare')  #BestPractice: set the current working directory\n",
    "filename = 'DataIku_app/dataIku_srv_access.xlsx'  #filename\n",
    "\n",
    "download_file = io.BytesIO()  #create memory object\n",
    "ftp.retrbinary(\"RETR {}\".format(filename), download_file.write)  #retrieve file from ftp\n",
    "download_file.seek(0)  #load file into memory\n",
    "\n",
    "ref = pd.read_excel(download_file.read())  #read virtual file into pandas\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ftp_read(f):\n",
    "    download_file = io.BytesIO()  #create memory object\n",
    "    ftp.retrbinary(\"RETR {}\".format(f), download_file.write)  #retrieve file from ftp\n",
    "    download_file.seek(0)\n",
    "    return download_file\n",
    "\n",
    "# ftp_read('DataIku_app/dataIku_srv_access.xlsx').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_b3'>2.b3 Write df to ftp</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ftp.cwd('/_Phi_Datascienceshare')  #BestPractice: set the working directory to write to\n",
    "# ftp.dir()\n",
    "\n",
    "df = ref.copy()\n",
    "\n",
    "# write data to ftp\n",
    "buffer = io.StringIO()  #create textwrapper object\n",
    "df.to_csv(buffer)  #write data to textwrapper\n",
    "bio = io.BytesIO(str.encode(buffer.getvalue()))  #convert txt into byte object\n",
    "\n",
    "filename = 'test_file.csv'\n",
    "ftp.storbinary(f'STOR /_Phi_Datascienceshare/{filename}', bio)  #write data to ftp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='2_b4'>2.b4 Move data between ftp folder</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "# new subfolder name\n",
    "fld = '{0}-{1}'.format(now.year, str(now.month).zfill(2))\n",
    "WellHealth = dataiku.Folder(\"4gHKVHeJ\")  #exiting folder in the flow \n",
    "files = [x[1:] for x in WellHealth.list_paths_in_partition() if fld in x]  #getting list of specific files for transfer\n",
    "\n",
    "def movef(filez):\n",
    "    for f in filez:\n",
    "        ftp.sendcmd(f'RNFR {src}{f}')  #rename file from\n",
    "        ftp.sendcmd(f'RNTO {path}{f}')  #rename file to\n",
    "    return print('Status: Transfer complete.')\n",
    "\n",
    "\n",
    "src = '/_FTP_Vendor_wellhealth/Referral Messages/'\n",
    "des = '/_PHI_EnterpriseReferralManagement/zdata/'\n",
    "\n",
    "path = f'{des}{fld}/'  #new path\n",
    "print(path)\n",
    "\n",
    "try: #create new subfolder if not existed\n",
    "    ftp.mkd(path)\n",
    "except: pass\n",
    "\n",
    "if len(files) > 0: movef(files)  #move file if it exist\n",
    "else: print('Status: No file available to transfer.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><a href='#toc' style='text-decoration:none;font-weight:bold;color:#0877ff;'>&#11014;&#65039; Back to the Top</a></div>\n",
    "\n",
    "# <a name='3_0'>3. Snowflake Connection</a>\n",
    "## <a name='3_a'>3.a Option 1: Dataiku API</a>\n",
    "\n",
    "### <a name='3_a1'>3.a1 Read from Snowflake</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://developer.dataiku.com/latest/api-reference/python/sql.html#dataiku.SQLExecutor2\n",
    "from dataiku.core.sql import SQLExecutor2\n",
    "\n",
    "exe = SQLExecutor2(connection=\"sf_write\")\n",
    "\n",
    "qr = \"\"\"SELECT * FROM MEMORIALHERMANN_DB.MEMORIAL_HERMANN_PROD.ENCOUNTER limit 500\"\"\"\n",
    "\n",
    "exe.query_to_df(qr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name='3_a2'>3.a2 Write to Snowflake</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfobj = dataiku.Dataset('tablename_inFlow')  #retrieve sf dataset obj\n",
    "dapi.write_with_schema(df)  #write BMA data to sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><a href='#toc' style='text-decoration:none;font-weight:bold;color:#0877ff;'>&#11014;&#65039; Back to the Top</a></div>\n",
    "\n",
    "## <a name='3_b'>3.b Option 2: Python Snowflake Connector</a>\n",
    "### 3.b1 Established connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.connector import connect\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "# https://stephenallwright.com/python-connector-write-pandas-snowflake/\n",
    "# https://docs.snowflake.com/ko/developer-guide/snowpark/reference/python/api/snowflake.snowpark.Session.write_pandas.html\n",
    "\n",
    "# create snowflake connection\n",
    "sf = dcon['sf_write']['params']  #retrieved connection info as py obj\n",
    "# sf = dcon['DS_DATAIKU_SCHEMA']['params']\n",
    "sf_usr = sf['user']  #retrieved connection's username\n",
    "sf_rol = sf['role']  #retrieved connection's role\n",
    "sf_pss = [k['value'] for k in ai[\"secrets\"] if k['key'] == sf_usr][0]   #retrieved connection's password\n",
    "sf_wrh = sf['warehouse']  #retrieved connection's warehouse\n",
    "\n",
    "\n",
    "conn = connect(user = sf_usr, password = sf_pss, warehouse = sf_wrh, role = sf_rol,\n",
    "               account = 'cerner-healtheedw_memorialhermann',)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b2 Read from snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Read ***\n",
    "def read_sn(sq, con):\n",
    "    # https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-api#fetch_pandas_all\n",
    "    cur = con.cursor()\n",
    "    cur.execute(sq)\n",
    "    return cur.fetch_pandas_all()\n",
    "\n",
    "sql = \"\"\"SELECT * FROM MEMORIALHERMANN_DB.MEMORIAL_HERMANN_PROD.ENCOUNTER LIMIT 1000\"\"\"\n",
    "read_sn(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b3 Write to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Write ***\n",
    "# https://www.mobilize.net/blog/creating-a-table-in-snowflake-using-the-python-connector\n",
    "\n",
    "def fix_pddate(df, tz='UTC'):  #fixed datetime issue between pandas and snowflake\n",
    "    # https://stackoverflow.com/a/70834485\n",
    "    tb = df.copy()\n",
    "    cols = tb.select_dtypes(include=['datetime64[ns]']).columns  #identify all datetime col\n",
    "    for col in cols:\n",
    "        tb[col] = tb[col].dt.tz_localize(tz)  #convert to local timezone datetime\n",
    "    return tb\n",
    "\n",
    "def sql_typ(dx, tbn):\n",
    "    #standardize object type to str\n",
    "    cols = dx.select_dtypes(include=['object']).columns\n",
    "    df = dx.astype({x:'str' for x in cols})\n",
    "    \n",
    "    # https://www.mobilize.net/blog/creating-a-table-in-snowflake-using-the-python-connector\n",
    "    #from pandas to snowflake datatype\n",
    "    tp = pd.DataFrame(df.dtypes, columns=['type'])\n",
    "    tp.index = tp.index.str.replace(' ','_')\n",
    "    tc = {'object':'varchar(500)','int64':'int','float64':'float','datetime64[ns]':'datetime','bool':'boolean'}\n",
    "    tp['type'] = tp['type'].replace(tc)\n",
    "\n",
    "    #create snowflake table schema\n",
    "    tx = [f'{x} {y}' for x,y in tp.to_dict()['type'].items()]\n",
    "    tx1 = ', '.join(tx)\n",
    "    db, sh = 'MHHS_DSCIENCE_DB','MHHS_DSCIENCE_WRITE'\n",
    "\n",
    "    sql = f\"\"\"create or replace table {db}.{sh}.{tbn} ({tx1})\"\"\"\n",
    "    cur.execute(sql)\n",
    "    tb = fix_pddate(df)  #fix pandas-2-snowflake datetime issue\n",
    "\n",
    "    #write data to newly created table\n",
    "    write_pandas(conn=conn, df=tb, table_name=tbn, database=db, schema=sh, overwrite=True)\n",
    "    return print('Snowflake table:', f'{db}.{sh}.{tbn}')\n",
    "\n",
    "# sql_typ(df, 'SF_table_name', conn)  #insert the table name you want to create/use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b4 Execute sql within snowflake only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_exc(sql, con):\n",
    "    cur = sql_exc.cursor()\n",
    "    return cur.execute(sql)\n",
    "\n",
    "sql1 = \"\"\"CREATE OR replace TABLE MHHS_DSCIENCE_DB.MHHS_DSCIENCE_WRITE.test_table AS\n",
    "\n",
    "SELECT DISTINCT d.icd10 code, d.obstetrics, d.perinatal, d.POSTPARTUM, d.gyn, d.pediatrics, d.NORMAL_NEWBORN, d.neonate, d.fetal, d.obpop, 'dx' src_cdx\n",
    "FROM MHHS_DSCIENCE_DB.MHHS_DSCIENCE_WRITE.WCSL_Pop_ref_Dx d\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT DISTINCT p.icd10 code, p.obstetrics, p.perinatal, p.POSTPARTUM, p.gyn, p.pediatrics, p.NORMAL_NEWBORN, p.neonate, p.fetal, p.obpop, 'pc' src_cdx\n",
    "FROM MHHS_DSCIENCE_DB.MHHS_DSCIENCE_WRITE.WCSL_Pop_ref_PCS p\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT DISTINCT c.cpt code, c.obstetrics, c.perinatal, c.POSTPARTUM, c.gyn, c.pediatrics, c.NORMAL_NEWBORN, c.neonate, c.fetal, c.obpop, 'cpt' src_cdx\n",
    "FROM MHHS_DSCIENCE_DB.MHHS_DSCIENCE_WRITE.WCSL_Pop_ref_CPT c\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT DISTINCT r.MSDRG code, r.obstetrics, r.perinatal, r.POSTPARTUM, r.gyn, r.pediatrics, r.NORMAL_NEWBORN, r.neonate, r.fetal, r.obpop, 'msdrg' src_cdx\n",
    "FROM MHHS_DSCIENCE_DB.MHHS_DSCIENCE_WRITE.WCSL_Pop_ref_DRG r\n",
    " \"\"\"\n",
    "\n",
    "# sql_exc(sql1, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2 = \"\"\"Drop table MHHS_DSCIENCE_DB.MHHS_DSCIENCE_WRITE.test_table\"\"\"\n",
    "# sql_exc(sql2, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align='right'><a href='#toc' style='text-decoration:none;font-weight:bold;color:#0877ff;'>&#11014;&#65039; Back to the Top</a></div>\n",
    "\n",
    "# <a name='4_0'>4. SQL Connection (Dataiku API)</a>\n",
    "\n",
    "## <a name='4_a'>4.a MySQL Connection</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use existing sql connection\n",
    "exe1 = sql_exc(connection=\"DSci_BMA_conn\")\n",
    "\n",
    "exe1.query_to_df(\"\"\"select * from [bma_NursingStation]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1691527855079,
  "creator": "E1724299",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env DSci_CodeEnv)",
   "language": "python",
   "name": "py-dku-venv-dsci_codeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "modifiedBy": "HeSaenz",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
